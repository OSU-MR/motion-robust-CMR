classdef GaussMixEstimOut < EstimOut
    % GaussMixEstimOut:  Gaussian Mixture output estimation
    %
    % Corresponds to an output channel of the form
    %   y = z + q
    %
    % Where q has the density
    % p(q) = (1-lambda) Nor(0,nu0) + lambda Nor(0,nu1)
    
    properties
        
        % Measured data
        Y;
        
        %Variances
        nu0;
        nu1;
        
        %on probability
        lambda;
        
        
        
        
    end
    
    methods
        % Constructor
        function obj = GaussMixEstimOut(Y,nu0,nu1,lambda)
            obj = obj@EstimOut;
            if nargin ~= 0 % Allow nargin == 0 syntax
                obj.Y = Y;
                obj.nu0 = nu0;
                obj.nu1 = nu1;
                obj.lambda = lambda;
                
                
                %Warn user that logLike has not been implemented yet
                %warning(['The logLike method is currently a beta version for this' ...
                %   ' class. It should be used with caution for adaptive step size'...
                %  ' until this problem is rectified']) %#ok<WNTAG>
            end
        end
        
        
        
        % Estimation function
        % Computes the posterior mean and variance given the estimates phat
        % and pvar
        function [zhat, zvar, p1] = estim(obj, phat, pvar)
            
            
            %Compute the intrinsic LLR
            int_llr = log(obj.lambda./(1 - obj.lambda));
            
            %Compute the extrinsic LLR
            ext_llr = (1/2)*log( (obj.nu0 + pvar) ./ (obj.nu1 + pvar)) ...
                + 0.5*(obj.Y - phat).^2 .*...
                ( 1 ./ (obj.nu0 + pvar) - 1./ (obj.nu1 + pvar));
            
            %Limit ext_llr
            ext_llr = min(100,max(-100,ext_llr));
            
            %Now compute p1 and p0
            p1 = 1 ./ (1 + exp(-int_llr - ext_llr) + eps);
            
            
            %Compute p0
            p0 = 1 - p1;
            
            %We can now obtain the mean
            zeta0 = (obj.Y .* pvar + phat.*obj.nu0) ./ ...
                (obj.nu0 + pvar);
            
            zeta1 = (obj.Y .* pvar + phat.*obj.nu1) ./ ...
                (obj.nu1 + pvar);
            
            zhat = p0.*zeta0 + p1.*zeta1;
            
            %Compute variance
            zvar = p0.*(obj.nu0.*pvar ./ (obj.nu0 + pvar) + zeta0.^2)...
                + p1.*(obj.nu1.*pvar ./ (obj.nu1 + pvar) + zeta1.^2)...
                - zhat.^2;
            
            
            %Protect form zvar getting too large or negative
            rval = 0.99;
            zvar = max(min(zvar,rval*pvar),0);
            
            
            
            
        end
        
        
        
        % Compute log likelihood
        %   E( log p_{Y|Z}(y|z) )
        function ll = logLike(obj,zhat,zvar)
            
            %Here we implement a lower bound to the expected log
            %likelihood. We exploit the identity
            %log(a+b) = log(a) + log(1 + b/a) and then approximate
            %log(1 + b/a) by log(b/a) where we limit the domain of
            %integration for this term to the region where b/a > 1.
            
            
            %Define cdf for Gaussian
            std_normal_cdf = @(x) 1/2*(1 + erf(x/sqrt(2)));
            
            %Define density of Gaussian
            std_normal_pdf = @(x) 1/sqrt(2*pi)*exp(-x.^2/2);
            
            
            %First, compute the nu1 term analytically
            ll = log(obj.lambda./sqrt(2.*pi.*obj.nu1)) -...
                1./(2.*obj.nu1).*(zvar + (zhat-obj.Y).^2);
            
            %Determine bounds of integration. We will integrate from
            %y - alpha to y + alpha
            alpha = sqrt(max(0,2.*log((1-obj.lambda).*sqrt(obj.nu1)./ ...
                (obj.lambda)./sqrt(obj.nu0)).* ...
                obj.nu0.*obj.nu1./(obj.nu1 - obj.nu0)));
            
            %Compute several required quantities
            pp1 = std_normal_pdf((obj.Y - alpha - zhat)./sqrt(zvar));
            pp2 = std_normal_pdf((obj.Y + alpha - zhat)./sqrt(zvar));
            c2 = std_normal_cdf((obj.Y - alpha - zhat)./sqrt(zvar));
            c1 = std_normal_cdf((obj.Y + alpha - zhat)./sqrt(zvar));
            beta = c1 - c2;
            
            %Compute mean and variance of the truncated normal distribution
            mval = zhat + sqrt(zvar).*(pp1 - pp2)./beta;
            vval = zvar.*(1 ...
                + ((obj.Y - alpha - zhat)./sqrt(zvar).*pp1 -...
                (obj.Y + alpha - zhat)./sqrt(zvar).*pp2)./beta...
                - ( (pp1 - pp2) ./ beta ).^2);
            
            %Compute the the nu0 term
            val = log((1 - obj.lambda).*sqrt(obj.nu1)./ ...
                obj.lambda./sqrt(obj.nu0)) +...
                (obj.nu0 - obj.nu1)./2./obj.nu0./obj.nu1.* ...
                (vval + (mval - obj.Y).^2);
            val = beta .* val;
            
            %Zero out entries where beta = 0
            val(beta == 0) = 0;
            
            %Combine the results
            ll = ll + val;
            
            
        end
        
        function S = numColumns(obj)
            %Return number of columns of Y
            S = size(obj.Y,2);
        end
        
        % Generate random samples from p(y|z)
        function y = genRand(obj, z)
            smallNoise = sqrt(obj.nu0).*randn(size(z));
            largeNoise = sqrt(obj.nu1).*randn(size(z));
            largeInds = (rand(size(z)) < obj.lambda);
            Noise = smallNoise;
            Noise(largeInds) = largeNoise(largeInds);
            y = z + Noise;
        end
    end
    
end

